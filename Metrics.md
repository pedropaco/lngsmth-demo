**Metrics**

***Answer Correctness.***
Evaluates the correctness of the LLM's response based on the expected output.\
*Ragas*

***Answer Relevancy.***
Evaluate the relevancy of an LLM's response based on the provided input and context.\
*DeepEval, Ragas*

***Aspect Critique.***
Evaluates specific dimensions or aspects of the generated content separately. Strict/binary metrics
such as: harmfulness, maliciousness, coherence, correctness, conciseness\
*Ragas*

***Bias.***
Evaluate the LLM's output for any inherent biases\
*DeepEval*

***Context Precision.***
Measure how accurately the generated response incorporates relevant context without introducing extraneous or incorrect information.\
*Deep Eval, Ragas*

***Context Relevancy.***
Measure how closely aligned with the contextual information the response is.\
*Deep Eval, Ragas*

***Context Recall.***
Measures how much of the relevant information from the provided context is included in the LLM's output.\
*Deep Eval, Ragas*

***Context Entities Recall.***
measures how well the LLM's output recalls specific entities present in the provided context\
*Ragas*

***Faithfulness.***
Evaluate whether the output generated by the LLM remains faithful to the
information provided in the context.
*Deep Eval, Ragas*

***Hallucination.***
detect and evaluate the presence of hallucinations in the LLM's output\
*Deep Eval*

***Knowledge Retention.***
Measures the LLM's ability to recall and retain information over time, often across multiple interactions.
*Deep Eval*

***Semantic Similarity.***
measures the similarity between the meanings of the actual output and the expected output or context
based on their semantic content.\
*Ragas*

***Summarization.***
Evaluate whether the summary generated by the LLM matches the expected summary
via the [Rouge metric](https://en.wikipedia.org/wiki/ROUGE_(metric)).\
*Deep Eval*

***Toxicity.***
evaluate the presence of toxic, harmful, or inappropriate content in the LLM's output.\
*Deep Eval*

***Custom Metrics.***
Ragas supports custom metrics BaseRagasLLM or BaseRagasEmbeddings.
DeepEval provides BaseMetric and GEval (LLM)

